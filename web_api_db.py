#!/usr/bin/env python3
"""
AI Model Pipeline Web API Service - ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“
æ¥æ”¶å®¢æˆ·ç«¯æä¾›çš„å®Œæ•´AIå¤„ç†æ•°æ®å¹¶ç›´æ¥ä¿å­˜åˆ°MySQLæ•°æ®åº“
"""

import json
import uuid
import logging
import time
from datetime import datetime
from typing import Optional, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from contextlib import asynccontextmanager

import mysql.connector
from mysql.connector import Error, pooling
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
import uvicorn

# é…ç½®
log_dir = Path("/tmp/ai-pipeline-logs")
log_dir.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_dir / 'web_api_service_direct.db.log')
    ]
)
logger = logging.getLogger(__name__)

DB_CONFIG = {
    'host': 'localhost',
    'database': 'ai_pipeline',
    'user': 'aiuser',
    'password': 'aipassword',
    'charset': 'utf8mb4'
}

MAX_WORKERS = 10

class BaseModelConfig:
    model_config = {"protected_namespaces": ()}

# æ•°æ®æ¨¡å‹ - åŒ…å«æ‰€æœ‰éœ€è¦ä¿å­˜åˆ°æ•°æ®åº“çš„å­—æ®µ
class ASRRequest(BaseModelConfig, BaseModel):
    audio_data: str = Field(..., description="Base64ç¼–ç çš„éŸ³é¢‘æ•°æ®æˆ–éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    session_id: str = Field(..., description="ä¼šè¯ID")
    language: str = Field("zh-CN", description="è¯­è¨€ä»£ç ")
    sample_rate: int = Field(16000, description="é‡‡æ ·ç‡")
    # ä»¥ä¸‹å­—æ®µç”±å®¢æˆ·ç«¯æä¾›ï¼Œå°†ä¿å­˜åˆ°æ•°æ®åº“
    output_data: str = Field(..., description="ASRè¾“å‡ºæ•°æ®ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰")
    processing_time_ms: int = Field(..., description="å¤„ç†æ—¶é—´(æ¯«ç§’)")
    timestamp: datetime = Field(..., description="æ—¶é—´æˆ³")
    status: str = Field("success", description="çŠ¶æ€: success, error, processing")
    metadata: Optional[Dict[str, Any]] = Field(None, description="å…ƒæ•°æ®")

class LLMRequest(BaseModelConfig, BaseModel):
    prompt: str = Field(..., description="è¾“å…¥æç¤º")
    session_id: str = Field(..., description="ä¼šè¯ID")
    max_tokens: int = Field(1024, description="æœ€å¤§tokenæ•°")
    temperature: float = Field(0.7, description="æ¸©åº¦å‚æ•°")
    model_name: str = Field("gpt-3.5-turbo", description="æ¨¡å‹åç§°")
    # ä»¥ä¸‹å­—æ®µç”±å®¢æˆ·ç«¯æä¾›ï¼Œå°†ä¿å­˜åˆ°æ•°æ®åº“
    output_data: str = Field(..., description="LLMè¾“å‡ºæ•°æ®ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰")
    processing_time_ms: int = Field(..., description="å¤„ç†æ—¶é—´(æ¯«ç§’)")
    timestamp: datetime = Field(..., description="æ—¶é—´æˆ³")
    status: str = Field("success", description="çŠ¶æ€: success, error, processing")
    metadata: Optional[Dict[str, Any]] = Field(None, description="å…ƒæ•°æ®")

class TTSRequest(BaseModelConfig, BaseModel):
    text: str = Field(..., description="è¦åˆæˆçš„æ–‡æœ¬")
    session_id: str = Field(..., description="ä¼šè¯ID")
    voice: str = Field("alloy", description="è¯­éŸ³ç±»å‹")
    speed: float = Field(1.0, description="è¯­é€Ÿ")
    audio_format: str = Field("mp3", description="éŸ³é¢‘æ ¼å¼")
    # ä»¥ä¸‹å­—æ®µç”±å®¢æˆ·ç«¯æä¾›ï¼Œå°†ä¿å­˜åˆ°æ•°æ®åº“
    output_data: str = Field(..., description="TTSè¾“å‡ºæ•°æ®ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰")
    processing_time_ms: int = Field(..., description="å¤„ç†æ—¶é—´(æ¯«ç§’)")
    timestamp: datetime = Field(..., description="æ—¶é—´æˆ³")
    status: str = Field("success", description="çŠ¶æ€: success, error, processing")
    metadata: Optional[Dict[str, Any]] = Field(None, description="å…ƒæ•°æ®")

# ç½‘å…³å“åº”æ¨¡å‹
class GatewayResponse(BaseModelConfig, BaseModel):
    session_id: str = Field(..., description="ä¼šè¯ID")
    model_type: str = Field(..., description="æ¨¡å‹ç±»å‹")
    status: str = Field(..., description="çŠ¶æ€")
    message: str = Field(..., description="å“åº”æ¶ˆæ¯")
    timestamp: datetime = Field(..., description="æ—¶é—´æˆ³")
    storage_method: str = Field(..., description="å­˜å‚¨æ–¹å¼")

# æ•°æ®åº“ç®¡ç†å™¨
class DatabaseManager:
    def __init__(self):
        self.connection_pool = None
        self._init_connection_pool()
    
    def _init_connection_pool(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
        try:
            self.connection_pool = pooling.MySQLConnectionPool(
                pool_name="web_api_db_pool",
                pool_size=20,
                **DB_CONFIG
            )
            logger.info("âœ… Database connection pool initialized successfully")
        except Error as e:
            logger.error(f"âŒ Failed to initialize connection pool: {e}")
            raise
    
    def save_message(self, message: Dict[str, Any]):
        """ä¿å­˜å•ä¸ªæ¶ˆæ¯åˆ°æ•°æ®åº“"""
        connection = None
        try:
            connection = self.connection_pool.get_connection()
            cursor = connection.cursor()
            
            logger.info(f"ğŸ”„ Attempting to save message for session: {message['session_id']}")
            
            # æ’å…¥AIæ¨¡å‹æ•°æ®
            insert_query = """
            INSERT INTO ai_model_data 
            (session_id, model_type, input_data, output_data, processing_time_ms, timestamp, status, metadata)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            # å‡†å¤‡æ•°æ®
            input_data = message.get('input_data', {})
            if not isinstance(input_data, str):
                input_data = json.dumps(input_data, ensure_ascii=False)
                
            output_data = message.get('output_data', '')
            if not isinstance(output_data, str):
                output_data = json.dumps(output_data, ensure_ascii=False)
            
            timestamp = message['timestamp']
            if isinstance(timestamp, str):
                timestamp = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            
            cursor.execute(insert_query, (
                message['session_id'],
                message['model_type'],
                input_data,
                output_data,
                message.get('processing_time_ms', 0),
                timestamp,
                message.get('status', 'success'),
                json.dumps(message.get('metadata', {}), ensure_ascii=False)
            ))
            
            # æ›´æ–°ä¼šè¯ä¿¡æ¯
            self._update_session(cursor, message)
            
            connection.commit()
            logger.info(f"âœ… Successfully saved message for session: {message['session_id']}")
            return True
            
        except Error as e:
            logger.error(f"âŒ Database error saving message: {e}")
            logger.error(f"Message data: {message}")
            if connection:
                connection.rollback()
            return False
        except Exception as e:
            logger.error(f"âŒ Unexpected error saving message: {e}")
            logger.error(f"Message data: {message}")
            import traceback
            logger.error(f"Stack trace: {traceback.format_exc()}")
            if connection:
                connection.rollback()
            return False
        finally:
            if connection and connection.is_connected():
                cursor.close()
                connection.close()
    
    def _update_session(self, cursor, message: Dict[str, Any]):
        """æ›´æ–°ä¼šè¯ä¿¡æ¯"""
        session_id = message['session_id']
        
        # æ£€æŸ¥ä¼šè¯æ˜¯å¦å­˜åœ¨
        check_query = "SELECT session_id FROM sessions WHERE session_id = %s"
        cursor.execute(check_query, (session_id,))
        
        timestamp = message['timestamp']
        if isinstance(timestamp, str):
            timestamp = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        
        if cursor.fetchone():
            # æ›´æ–°ç°æœ‰ä¼šè¯
            update_query = """
            UPDATE sessions 
            SET updated_at = %s, 
                total_processing_time_ms = total_processing_time_ms + %s
            WHERE session_id = %s
            """
            cursor.execute(update_query, (
                timestamp,
                message.get('processing_time_ms', 0),
                session_id
            ))
        else:
            # æ’å…¥æ–°ä¼šè¯
            insert_query = """
            INSERT INTO sessions (session_id, created_at, updated_at, total_processing_time_ms)
            VALUES (%s, %s, %s, %s)
            """
            cursor.execute(insert_query, (
                session_id,
                timestamp,
                timestamp,
                message.get('processing_time_ms', 0)
            ))
    
    def check_connection(self):
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸"""
        try:
            connection = self.connection_pool.get_connection()
            cursor = connection.cursor()
            cursor.execute("SELECT 1")
            cursor.close()
            connection.close()
            return True
        except Error as e:
            logger.error(f"Database connection check failed: {e}")
            return False

# å…¨å±€å˜é‡ï¼Œç”¨äºåœ¨åå°ä»»åŠ¡ä¸­è®¿é—®æ•°æ®åº“ç®¡ç†å™¨
db_manager = None

def _save_to_database_async(message: Dict[str, Any]):
    """å¼‚æ­¥ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“"""
    global db_manager
    try:
        logger.info(f"ğŸ”„ Starting async DB save for session: {message.get('session_id')}")
        success = db_manager.save_message(message)
        if success:
            logger.info(f"âœ… Async DB save successful for session: {message.get('session_id')}")
        else:
            logger.error(f"âŒ Async DB save failed for session: {message.get('session_id')}")
        return success
    except Exception as e:
        logger.error(f"âŒ Error in async DB save for session {message.get('session_id')}: {e}")
        import traceback
        logger.error(f"Stack trace: {traceback.format_exc()}")
        return False

# ç”Ÿå‘½å‘¨æœŸç®¡ç†
@asynccontextmanager
async def lifespan(app: FastAPI):
    global db_manager
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    db_manager = DatabaseManager()
    app.state.db_manager = db_manager
    app.state.thread_pool = ThreadPoolExecutor(max_workers=MAX_WORKERS)
    
    logger.info("API Gateway with direct DB storage startup complete")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    logger.info("API Gateway shutdown started")
    app.state.thread_pool.shutdown(wait=True)
    logger.info("API Gateway shutdown complete")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="AI Model Pipeline Data Collection API - Direct DB Storage",
    description="æ¥æ”¶å®¢æˆ·ç«¯æä¾›çš„å®Œæ•´AIå¤„ç†æ•°æ®å¹¶ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“",
    version="1.0.0",
    lifespan=lifespan
)

@app.post("/asr/process", response_model=GatewayResponse)
async def process_asr(request: ASRRequest, background_tasks: BackgroundTasks):
    """æ¥æ”¶ASRå¤„ç†ç»“æœå¹¶ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“"""
    try:
        # æ„å»ºæ•°æ®åº“æ¶ˆæ¯ - åŒ…å«æ‰€æœ‰éœ€è¦ä¿å­˜åˆ°æ•°æ®åº“çš„å­—æ®µ
        db_message = {
            "session_id": request.session_id,
            "model_type": "ASR",
            "input_data": {"audio_data": request.audio_data, "language": request.language, "sample_rate": request.sample_rate},
            "output_data": request.output_data,
            "processing_time_ms": request.processing_time_ms,
            "timestamp": request.timestamp.isoformat(),
            "status": request.status,
            "metadata": request.metadata or {}
        }
        
        # åœ¨åå°ä»»åŠ¡ä¸­ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“
        background_tasks.add_task(_save_to_database_async, db_message)
        
        logger.info(f"ASR data received for session {request.session_id} - queued for direct DB storage")
        
        return GatewayResponse(
            session_id=request.session_id,
            model_type="ASR",
            status="accepted",
            message="ASR data received and queued for direct database storage",
            timestamp=datetime.now(),
            storage_method="direct_database"
        )
        
    except Exception as e:
        logger.error(f"ASR data processing error for session {request.session_id}: {e}")
        raise HTTPException(status_code=500, detail=f"ASR data processing failed: {str(e)}")

@app.post("/llm/process", response_model=GatewayResponse)
async def process_llm(request: LLMRequest, background_tasks: BackgroundTasks):
    """æ¥æ”¶LLMå¤„ç†ç»“æœå¹¶ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“"""
    try:
        # æ„å»ºæ•°æ®åº“æ¶ˆæ¯ - åŒ…å«æ‰€æœ‰éœ€è¦ä¿å­˜åˆ°æ•°æ®åº“çš„å­—æ®µ
        db_message = {
            "session_id": request.session_id,
            "model_type": "LLM",
            "input_data": {"prompt": request.prompt, "max_tokens": request.max_tokens, "temperature": request.temperature, "model_name": request.model_name},
            "output_data": request.output_data,
            "processing_time_ms": request.processing_time_ms,
            "timestamp": request.timestamp.isoformat(),
            "status": request.status,
            "metadata": request.metadata or {}
        }
        
        # åœ¨åå°ä»»åŠ¡ä¸­ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“
        background_tasks.add_task(_save_to_database_async, db_message)
        
        logger.info(f"LLM data received for session {request.session_id} - queued for direct DB storage")
        
        return GatewayResponse(
            session_id=request.session_id,
            model_type="LLM",
            status="accepted",
            message="LLM data received and queued for direct database storage",
            timestamp=datetime.now(),
            storage_method="direct_database"
        )
        
    except Exception as e:
        logger.error(f"LLM data processing error for session {request.session_id}: {e}")
        raise HTTPException(status_code=500, detail=f"LLM data processing failed: {str(e)}")

@app.post("/tts/process", response_model=GatewayResponse)
async def process_tts(request: TTSRequest, background_tasks: BackgroundTasks):
    """æ¥æ”¶TTSå¤„ç†ç»“æœå¹¶ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“"""
    try:
        # æ„å»ºæ•°æ®åº“æ¶ˆæ¯ - åŒ…å«æ‰€æœ‰éœ€è¦ä¿å­˜åˆ°æ•°æ®åº“çš„å­—æ®µ
        db_message = {
            "session_id": request.session_id,
            "model_type": "TTS",
            "input_data": {"text": request.text, "voice": request.voice, "speed": request.speed, "audio_format": request.audio_format},
            "output_data": request.output_data,
            "processing_time_ms": request.processing_time_ms,
            "timestamp": request.timestamp.isoformat(),
            "status": request.status,
            "metadata": request.metadata or {}
        }
        
        # åœ¨åå°ä»»åŠ¡ä¸­ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“
        background_tasks.add_task(_save_to_database_async, db_message)
        
        logger.info(f"TTS data received for session {request.session_id} - queued for direct DB storage")
        
        return GatewayResponse(
            session_id=request.session_id,
            model_type="TTS",
            status="accepted",
            message="TTS data received and queued for direct database storage",
            timestamp=datetime.now(),
            storage_method="direct_database"
        )
        
    except Exception as e:
        logger.error(f"TTS data processing error for session {request.session_id}: {e}")
        raise HTTPException(status_code=500, detail=f"TTS data processing failed: {str(e)}")

@app.post("/llm/process-sync", response_model=GatewayResponse)
async def process_llm_sync(request: LLMRequest):
    """åŒæ­¥ç‰ˆæœ¬ï¼šæ¥æ”¶LLMå¤„ç†ç»“æœå¹¶ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    try:
        # æ„å»ºæ•°æ®åº“æ¶ˆæ¯
        db_message = {
            "session_id": request.session_id,
            "model_type": "LLM",
            "input_data": {"prompt": request.prompt, "max_tokens": request.max_tokens, "temperature": request.temperature, "model_name": request.model_name},
            "output_data": request.output_data,
            "processing_time_ms": request.processing_time_ms,
            "timestamp": request.timestamp.isoformat(),
            "status": request.status,
            "metadata": request.metadata or {}
        }
        
        # åŒæ­¥ä¿å­˜åˆ°æ•°æ®åº“
        success = app.state.db_manager.save_message(db_message)
        
        if success:
            logger.info(f"âœ… Sync DB save successful for session {request.session_id}")
            return GatewayResponse(
                session_id=request.session_id,
                model_type="LLM",
                status="saved",
                message="LLM data successfully saved to database",
                timestamp=datetime.now(),
                storage_method="direct_database_sync"
            )
        else:
            logger.error(f"âŒ Sync DB save failed for session {request.session_id}")
            raise HTTPException(status_code=500, detail="Failed to save data to database")
        
    except Exception as e:
        logger.error(f"LLM sync data processing error for session {request.session_id}: {e}")
        raise HTTPException(status_code=500, detail=f"LLM data processing failed: {str(e)}")

@app.get("/debug/db-check")
async def debug_db_check():
    """è°ƒè¯•ç«¯ç‚¹ï¼šæ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œè¡¨ç»“æ„"""
    try:
        connection = app.state.db_manager.connection_pool.get_connection()
        cursor = connection.cursor()
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute("SHOW TABLES LIKE 'ai_model_data'")
        ai_table_exists = cursor.fetchone() is not None
        
        cursor.execute("SHOW TABLES LIKE 'sessions'")
        sessions_table_exists = cursor.fetchone() is not None
        
        # è·å–å®é™…çš„è¡¨ç»“æ„
        ai_columns = []
        if ai_table_exists:
            cursor.execute("DESCRIBE ai_model_data")
            ai_columns = [column[0] for column in cursor.fetchall()]
        
        sessions_columns = []
        if sessions_table_exists:
            cursor.execute("DESCRIBE sessions")
            sessions_columns = [column[0] for column in cursor.fetchall()]
        
        # æ£€æŸ¥æ•°æ®æ•°é‡
        cursor.execute("SELECT COUNT(*) as count FROM ai_model_data")
        total_records = cursor.fetchone()[0]
        
        # ä½¿ç”¨timestampå­—æ®µæŸ¥è¯¢æœ€è¿‘çš„æ•°æ®
        if ai_table_exists and 'timestamp' in ai_columns:
            cursor.execute("SELECT session_id, model_type, timestamp FROM ai_model_data ORDER BY timestamp DESC LIMIT 5")
            recent_records = cursor.fetchall()
        else:
            recent_records = []
        
        cursor.close()
        connection.close()
        
        return {
            "database_connected": True,
            "tables": {
                "ai_model_data": ai_table_exists,
                "sessions": sessions_table_exists
            },
            "ai_model_data_columns": ai_columns,
            "sessions_columns": sessions_columns,
            "total_records": total_records,
            "recent_records": recent_records
        }
        
    except Exception as e:
        logger.error(f"Debug DB check failed: {e}")
        return {
            "database_connected": False,
            "error": str(e)
        }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    db_healthy = (hasattr(app.state, 'db_manager') and 
                 app.state.db_manager.check_connection())
    
    return {
        "status": "healthy" if db_healthy else "degraded",
        "timestamp": datetime.now().isoformat(),
        "database_connected": db_healthy,
        "storage_method": "direct_database",
        "service": "ai_data_collection_gateway_direct_db"
    }

@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹"""
    return {
        "message": "AI Model Pipeline Data Collection API - Direct DB Storage",
        "version": "1.0.0",
        "description": "æ¥æ”¶å®¢æˆ·ç«¯æä¾›çš„å®Œæ•´AIå¤„ç†æ•°æ®å¹¶ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“",
        "storage_method": "direct_database",
        "endpoints": {
            "asr": "/asr/process",
            "llm": "/llm/process", 
            "tts": "/tts/process",
            "llm_sync_debug": "/llm/process-sync",
            "debug_db": "/debug/db-check",
            "health": "/health"
        }
    }

if __name__ == "__main__":
    uvicorn.run(
        app,  # ç›´æ¥ä½¿ç”¨appå¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )